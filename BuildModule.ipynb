{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvF0RHQ7_V5Q"
      },
      "source": [
        "# 神经网络\n",
        "\n",
        "将对于数据的操作的各个模块或层次组织在一起，就是神经网络模型。`torch.nn`模块提供了所有的用于建立神经网络模型的模块的命名空间(隐藏层、输出层等)。所有的模型都是`nn.Moudle`的子类。一个神经网络模型可以包括许多的其他的模型，这样就可以建立很复杂的结构。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jOKaRrX_SDd",
        "outputId": "590cdb03-4fff-40e9-f61d-63f57bd7a388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "计算运行设备: cuda\n"
          ]
        }
      ],
      "source": [
        "# 导入相应的包\n",
        "import torch\n",
        "import os\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# 获取可以执行的设备\n",
        "print(\"计算运行设备:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLj1AMvF_b_e"
      },
      "source": [
        "# 建立模型\n",
        "可以通过继承`nn.Module`类来实现模型的建立，然后在`__init__`函数里初始化建立模型。所有的Module子类都需要实现前向传播函数`forward`,来实现对输入数据的处理。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvlM7wqj_aAq",
        "outputId": "347f5c86-ab7c-4130-b77e-4d515d558ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork,self).__init__()\n",
        "        self.flatten = nn.Flatten()   # 将图像扁平化\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)  # 获取每一batch的输入，并利用第一个模块扁平化\n",
        "        logits = self.linear_relu_stack(x) # 放入第二个复杂的模型进行训练\n",
        "        return logits\n",
        "    \n",
        "# 将模型放到对应的结构上\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsjjmv6B0dQ"
      },
      "source": [
        "# 使用模型\n",
        "为了使用模型，可以直接向模型传递输入数据，上面的模型最后将会输出一个10维的`tensor`结构，这些操作都是在后台进行的，无需显式的调用`forward`函数进行计算。因为这是一个多分类问题，将上述的10维的结构放入一个`softmax`分类器里面，就可以得到概率最大的那个类别，也就是最终的分类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K6kp62kBavp",
        "outputId": "da8e920d-1bf8-43d1-9d2d-1b962a26b64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x tensor([[[8.3332e-01, 6.4700e-01, 2.6904e-01, 9.4117e-01, 8.3041e-01,\n",
            "          6.8950e-01, 7.5871e-01, 7.2565e-01, 3.7627e-01, 7.5096e-01,\n",
            "          2.8476e-01, 5.8763e-01, 3.6398e-01, 6.8863e-01, 4.3805e-01,\n",
            "          2.2661e-01, 6.4731e-01, 3.0593e-01, 7.2615e-01, 8.8779e-01,\n",
            "          9.0504e-01, 4.8914e-01, 5.3835e-01, 3.1738e-01, 3.1175e-01,\n",
            "          1.6134e-01, 2.9150e-01, 1.7263e-02],\n",
            "         [7.9437e-01, 6.8342e-01, 3.2559e-01, 6.7508e-01, 6.9991e-01,\n",
            "          6.6925e-01, 7.2385e-01, 8.4121e-02, 4.8750e-01, 5.2389e-01,\n",
            "          2.9232e-01, 5.5563e-01, 4.7164e-01, 3.9739e-01, 5.1104e-01,\n",
            "          5.8132e-03, 4.8609e-01, 4.9585e-01, 8.2917e-01, 8.7852e-02,\n",
            "          2.8830e-01, 5.9997e-01, 1.5156e-01, 1.5843e-02, 4.6177e-01,\n",
            "          3.5969e-01, 9.1840e-01, 2.1088e-01],\n",
            "         [9.1114e-01, 4.1642e-04, 2.7588e-01, 4.5763e-02, 3.0718e-01,\n",
            "          2.1817e-01, 6.5364e-01, 5.1978e-01, 2.8948e-01, 7.5754e-01,\n",
            "          2.3738e-01, 9.6432e-01, 3.2489e-01, 2.4330e-01, 6.0595e-01,\n",
            "          2.2734e-01, 6.2581e-01, 7.4639e-01, 4.6131e-01, 3.0206e-01,\n",
            "          1.0231e-01, 9.0318e-01, 7.6777e-01, 2.6004e-01, 2.8443e-01,\n",
            "          6.7480e-01, 1.8994e-01, 5.4989e-01],\n",
            "         [3.7257e-01, 5.9800e-01, 4.3425e-01, 1.6912e-01, 9.4808e-01,\n",
            "          9.9638e-01, 7.8843e-02, 3.3132e-01, 8.8951e-01, 5.3417e-01,\n",
            "          6.1774e-01, 1.6422e-01, 3.1206e-01, 9.1849e-01, 3.4036e-01,\n",
            "          1.8207e-01, 7.9982e-01, 2.3976e-02, 6.8358e-01, 8.2321e-01,\n",
            "          2.5994e-01, 8.9780e-01, 2.2917e-01, 6.4324e-01, 3.0190e-01,\n",
            "          9.1199e-01, 7.0220e-01, 2.2318e-01],\n",
            "         [6.2587e-01, 9.5838e-01, 7.1999e-01, 3.3705e-01, 3.7204e-01,\n",
            "          2.9528e-01, 3.4735e-01, 4.9859e-01, 8.1055e-02, 9.7373e-01,\n",
            "          8.6646e-01, 1.9269e-01, 8.1838e-01, 1.0020e-01, 4.1906e-01,\n",
            "          8.2067e-01, 5.4729e-01, 6.0529e-01, 2.3837e-01, 9.2306e-02,\n",
            "          3.9419e-03, 3.8899e-01, 3.9142e-01, 5.1251e-01, 2.1341e-01,\n",
            "          3.6599e-01, 9.1168e-01, 3.9345e-01],\n",
            "         [9.6247e-01, 1.9927e-01, 2.7921e-01, 9.6898e-01, 6.4865e-01,\n",
            "          7.2293e-01, 4.7753e-01, 6.9831e-01, 7.8276e-01, 3.3874e-01,\n",
            "          9.0368e-01, 7.3486e-01, 4.2977e-01, 1.4639e-01, 8.8625e-01,\n",
            "          7.0513e-01, 6.7069e-02, 8.0426e-01, 4.6732e-01, 2.7174e-01,\n",
            "          5.1146e-01, 5.6790e-01, 3.5619e-01, 5.6413e-01, 9.7176e-01,\n",
            "          1.9228e-01, 6.9469e-01, 8.0617e-01],\n",
            "         [4.4265e-01, 7.8787e-01, 8.9331e-01, 3.1411e-01, 3.2564e-01,\n",
            "          3.4148e-02, 1.2322e-01, 5.9589e-01, 2.0930e-01, 5.0907e-01,\n",
            "          4.7715e-01, 6.6118e-01, 5.6916e-01, 4.1698e-01, 8.7215e-01,\n",
            "          7.9926e-01, 5.7508e-01, 8.2183e-01, 9.5096e-01, 4.0319e-01,\n",
            "          9.9615e-01, 4.6472e-01, 5.7676e-01, 1.5989e-01, 1.8387e-01,\n",
            "          9.6163e-01, 3.7328e-01, 1.8933e-01],\n",
            "         [4.9979e-01, 2.0940e-01, 4.8111e-01, 7.0521e-01, 1.6121e-01,\n",
            "          5.1285e-01, 5.0352e-01, 9.5132e-01, 5.7321e-01, 8.7960e-02,\n",
            "          3.5384e-01, 8.5257e-01, 1.6014e-02, 6.1879e-01, 5.1339e-01,\n",
            "          5.8178e-01, 9.9517e-01, 9.2044e-01, 6.3737e-01, 4.6736e-02,\n",
            "          4.1834e-01, 5.5699e-01, 4.2405e-01, 9.2163e-02, 7.3853e-01,\n",
            "          8.9925e-01, 4.6114e-01, 9.9739e-02],\n",
            "         [3.8250e-01, 5.2838e-01, 3.6460e-01, 3.3221e-01, 4.2742e-01,\n",
            "          5.1509e-01, 1.6518e-01, 7.0752e-02, 3.9949e-01, 5.8630e-02,\n",
            "          4.9321e-01, 6.8198e-01, 6.5139e-02, 3.0686e-01, 1.9239e-01,\n",
            "          8.5537e-01, 2.1320e-01, 7.7869e-01, 1.8016e-01, 1.8262e-01,\n",
            "          3.1445e-01, 6.0272e-01, 3.5807e-01, 1.2957e-01, 7.5442e-01,\n",
            "          6.4135e-01, 3.2092e-01, 5.3789e-01],\n",
            "         [7.6869e-01, 3.2352e-01, 8.4331e-01, 7.0927e-01, 2.7954e-01,\n",
            "          7.9309e-01, 7.5539e-01, 9.7469e-01, 6.8045e-01, 7.5019e-01,\n",
            "          7.7357e-01, 8.4832e-03, 2.8016e-01, 7.1944e-01, 1.4099e-01,\n",
            "          3.2897e-02, 4.2332e-01, 4.3168e-01, 6.4767e-02, 5.7506e-01,\n",
            "          9.8358e-01, 1.9783e-01, 8.5825e-01, 7.9030e-01, 1.2501e-01,\n",
            "          7.0446e-01, 8.6552e-01, 5.7665e-01],\n",
            "         [4.5847e-02, 7.2697e-01, 8.7702e-01, 2.5932e-01, 9.1903e-01,\n",
            "          2.8931e-01, 7.5884e-01, 9.7002e-01, 4.1348e-02, 5.2818e-01,\n",
            "          9.9864e-01, 4.8623e-02, 9.2123e-01, 5.7897e-01, 9.5965e-01,\n",
            "          7.9875e-02, 5.9005e-01, 1.0785e-01, 8.5150e-01, 7.8178e-01,\n",
            "          9.2043e-01, 9.5910e-01, 2.8549e-01, 6.6814e-01, 5.6079e-01,\n",
            "          9.1453e-01, 7.6942e-01, 5.2929e-01],\n",
            "         [1.3672e-01, 1.2873e-01, 5.8607e-01, 4.0932e-01, 4.9968e-01,\n",
            "          6.9125e-02, 3.7696e-01, 3.6766e-01, 6.2861e-02, 9.3338e-02,\n",
            "          3.5826e-01, 7.8693e-01, 2.6156e-01, 6.4117e-01, 5.4045e-01,\n",
            "          2.5948e-01, 3.1615e-01, 9.4130e-01, 1.3500e-01, 1.1300e-01,\n",
            "          3.8339e-01, 5.2802e-01, 5.5613e-01, 1.2997e-01, 9.6869e-01,\n",
            "          2.4544e-01, 4.5286e-01, 3.0952e-01],\n",
            "         [7.7992e-02, 8.0962e-01, 4.2245e-01, 7.2288e-01, 3.4656e-01,\n",
            "          9.1610e-01, 5.5800e-01, 2.5254e-01, 6.6930e-01, 4.1516e-01,\n",
            "          7.8618e-01, 8.2729e-01, 1.6564e-01, 6.1830e-01, 5.0424e-01,\n",
            "          8.4269e-01, 9.7843e-02, 2.1654e-01, 3.8031e-01, 9.5119e-01,\n",
            "          8.8278e-01, 4.9729e-01, 4.2583e-01, 5.8782e-01, 1.3310e-01,\n",
            "          7.2900e-01, 3.3507e-01, 1.0859e-01],\n",
            "         [1.2934e-01, 1.3074e-02, 9.6421e-01, 6.2947e-01, 6.5526e-01,\n",
            "          2.1880e-01, 8.3194e-01, 5.6721e-01, 9.9999e-01, 1.8743e-01,\n",
            "          2.6582e-01, 9.7842e-01, 7.3622e-01, 5.9214e-01, 2.4537e-01,\n",
            "          9.0636e-01, 4.5441e-01, 2.3984e-01, 9.3631e-01, 3.7702e-01,\n",
            "          3.3404e-01, 4.3651e-01, 7.7220e-01, 3.5401e-01, 4.9769e-01,\n",
            "          2.8354e-01, 3.9111e-01, 3.5956e-01],\n",
            "         [1.0568e-01, 7.6559e-01, 4.2669e-01, 7.0931e-01, 4.6872e-01,\n",
            "          5.2021e-01, 6.5425e-01, 4.3369e-03, 3.7008e-01, 2.8320e-02,\n",
            "          9.7134e-01, 1.6132e-01, 6.7083e-01, 9.7959e-01, 2.7862e-01,\n",
            "          4.0114e-01, 3.2779e-01, 4.6088e-01, 9.4637e-01, 7.9180e-01,\n",
            "          7.5822e-01, 7.5553e-02, 7.3938e-01, 9.6614e-01, 9.3087e-01,\n",
            "          3.8632e-01, 3.2274e-01, 3.6230e-01],\n",
            "         [5.2961e-01, 3.0196e-01, 7.6303e-01, 5.6479e-01, 3.6608e-01,\n",
            "          9.9113e-01, 7.6293e-02, 9.7743e-01, 5.3145e-01, 4.6615e-01,\n",
            "          1.7557e-01, 8.0383e-01, 7.0970e-01, 8.8090e-01, 3.1693e-01,\n",
            "          2.6772e-01, 1.7038e-02, 1.9873e-01, 8.6868e-01, 8.7972e-01,\n",
            "          3.4342e-01, 7.5613e-01, 2.1853e-01, 7.0847e-01, 9.8488e-01,\n",
            "          1.2424e-01, 3.7129e-02, 3.7303e-01],\n",
            "         [6.4117e-01, 6.4709e-01, 5.5902e-01, 8.4646e-01, 3.3233e-01,\n",
            "          1.1943e-01, 9.7757e-01, 2.8270e-01, 3.6024e-01, 2.1050e-02,\n",
            "          9.6768e-01, 5.0508e-01, 4.4588e-01, 7.9797e-01, 5.6211e-01,\n",
            "          8.6250e-01, 2.8303e-01, 3.4608e-01, 5.8875e-02, 8.3445e-01,\n",
            "          1.6685e-01, 8.6739e-01, 3.6326e-01, 9.4293e-01, 7.1227e-02,\n",
            "          7.3466e-01, 5.6588e-01, 1.5855e-01],\n",
            "         [9.0248e-01, 8.0561e-01, 9.6427e-01, 1.1451e-01, 3.2625e-01,\n",
            "          8.9916e-01, 2.1226e-01, 4.1665e-01, 7.6094e-01, 7.6312e-01,\n",
            "          9.3360e-02, 8.5729e-03, 6.5558e-01, 8.1209e-01, 1.0610e-01,\n",
            "          2.0979e-01, 4.1081e-01, 2.0621e-01, 2.2817e-01, 2.5701e-01,\n",
            "          8.6727e-01, 2.8406e-01, 6.9189e-01, 2.7093e-01, 9.7753e-01,\n",
            "          3.9624e-01, 1.4648e-01, 9.7059e-01],\n",
            "         [7.7510e-01, 3.9417e-01, 8.4908e-01, 7.5180e-01, 2.0450e-01,\n",
            "          6.7799e-01, 7.4712e-01, 2.5207e-01, 2.9634e-01, 2.1404e-01,\n",
            "          4.9501e-01, 7.8515e-01, 3.1007e-01, 3.7972e-01, 5.9373e-02,\n",
            "          2.9975e-01, 8.1352e-01, 4.3058e-01, 4.4011e-01, 5.9169e-01,\n",
            "          6.1470e-01, 2.1049e-01, 5.8524e-01, 1.1417e-01, 2.8747e-01,\n",
            "          6.7050e-01, 5.1293e-01, 1.1945e-01],\n",
            "         [3.7729e-01, 3.4210e-01, 6.2506e-02, 3.7201e-01, 3.5562e-01,\n",
            "          2.7707e-01, 6.9212e-01, 7.4128e-01, 6.5652e-01, 4.3517e-01,\n",
            "          9.9027e-02, 5.1247e-01, 4.1070e-01, 6.7815e-01, 1.7260e-01,\n",
            "          4.0767e-01, 1.2411e-01, 7.8330e-01, 9.4274e-01, 9.4300e-01,\n",
            "          8.1936e-01, 4.6664e-01, 4.1695e-01, 3.1560e-01, 1.9121e-01,\n",
            "          3.3207e-01, 8.3728e-01, 3.8842e-01],\n",
            "         [7.6767e-01, 1.9466e-01, 1.3838e-01, 4.9126e-01, 4.3767e-01,\n",
            "          4.8379e-01, 1.1156e-01, 5.0423e-01, 4.4954e-01, 8.3357e-01,\n",
            "          5.2606e-01, 1.8705e-01, 7.0129e-01, 5.5442e-01, 7.8510e-01,\n",
            "          7.8407e-01, 1.5484e-01, 8.7302e-01, 2.8340e-01, 1.8797e-01,\n",
            "          9.5755e-01, 9.3904e-01, 5.3837e-01, 5.1232e-01, 2.3928e-01,\n",
            "          9.2230e-01, 3.8229e-01, 9.9216e-01],\n",
            "         [5.6559e-01, 1.7393e-01, 6.9115e-01, 1.3087e-01, 5.6106e-02,\n",
            "          5.6215e-01, 4.9655e-01, 2.1354e-01, 2.4183e-01, 2.1234e-01,\n",
            "          1.9299e-01, 7.6574e-01, 5.6707e-01, 5.0597e-01, 3.1759e-02,\n",
            "          1.6482e-01, 8.9439e-01, 3.0495e-01, 1.7442e-01, 4.1298e-01,\n",
            "          2.6623e-01, 1.8101e-01, 8.5050e-01, 4.4168e-02, 2.0039e-01,\n",
            "          1.5112e-01, 1.9724e-01, 8.9250e-01],\n",
            "         [9.2760e-01, 6.3717e-01, 6.4020e-01, 4.2837e-01, 1.9189e-01,\n",
            "          9.1364e-01, 2.7109e-01, 1.9610e-01, 3.3400e-01, 7.4676e-01,\n",
            "          4.3157e-01, 9.4517e-01, 3.0225e-01, 9.2969e-01, 3.1766e-01,\n",
            "          6.1476e-01, 1.9692e-01, 2.6353e-01, 9.1357e-02, 1.5318e-01,\n",
            "          8.4444e-01, 7.2794e-02, 3.1712e-01, 9.0105e-01, 4.1753e-01,\n",
            "          5.9293e-01, 4.6627e-01, 2.3211e-01],\n",
            "         [7.5702e-01, 1.4906e-01, 2.3686e-02, 7.0553e-01, 1.9197e-01,\n",
            "          2.7605e-02, 9.2809e-01, 4.5136e-01, 9.8188e-01, 7.7652e-01,\n",
            "          7.2771e-01, 1.1035e-02, 5.4399e-01, 7.3032e-01, 5.2351e-01,\n",
            "          4.9096e-01, 6.8264e-01, 1.6624e-01, 5.9663e-01, 3.7747e-01,\n",
            "          1.0083e-01, 8.8289e-01, 2.3926e-01, 1.2606e-02, 2.8122e-01,\n",
            "          8.7988e-01, 7.9625e-01, 4.4845e-02],\n",
            "         [4.2447e-01, 9.6214e-01, 5.1572e-02, 3.8985e-01, 6.9061e-01,\n",
            "          2.0129e-01, 7.2436e-01, 9.6716e-01, 2.6290e-01, 7.4301e-01,\n",
            "          4.6164e-01, 3.5182e-01, 6.8633e-01, 7.3149e-01, 4.0804e-01,\n",
            "          6.8414e-01, 7.9863e-01, 2.7366e-02, 2.3525e-01, 8.7288e-01,\n",
            "          8.1087e-01, 9.4370e-01, 2.4576e-01, 1.4828e-01, 7.7594e-01,\n",
            "          1.4961e-01, 9.4771e-01, 1.1393e-01],\n",
            "         [6.7501e-02, 3.7252e-02, 2.6122e-01, 3.6396e-01, 2.1946e-01,\n",
            "          3.0435e-01, 3.4900e-01, 2.1628e-01, 4.5715e-01, 8.5644e-01,\n",
            "          1.2030e-01, 2.4375e-01, 2.3859e-01, 3.9507e-01, 7.8149e-01,\n",
            "          7.4216e-01, 3.3338e-01, 8.9788e-01, 3.9723e-01, 4.0239e-01,\n",
            "          9.3020e-01, 2.5518e-01, 2.8015e-01, 8.1089e-01, 6.3198e-01,\n",
            "          4.2839e-01, 6.0846e-01, 3.8728e-02],\n",
            "         [1.2260e-01, 2.0059e-01, 8.3039e-01, 7.6272e-02, 1.9570e-01,\n",
            "          2.9066e-01, 8.1021e-01, 6.5160e-01, 7.1473e-02, 8.9955e-01,\n",
            "          8.1228e-01, 1.4735e-01, 5.4040e-02, 1.6486e-01, 7.3687e-01,\n",
            "          2.1211e-01, 6.0485e-01, 2.9769e-01, 6.9807e-01, 3.3061e-01,\n",
            "          9.5823e-01, 2.4190e-01, 3.9111e-01, 9.2678e-01, 9.3307e-02,\n",
            "          2.4459e-01, 7.4323e-02, 5.0910e-01],\n",
            "         [3.0805e-01, 6.3870e-01, 9.4140e-01, 9.6632e-02, 4.2951e-01,\n",
            "          5.8734e-01, 9.1584e-01, 7.4371e-01, 2.1868e-01, 1.0010e-01,\n",
            "          6.6755e-01, 3.3421e-01, 8.6528e-01, 2.7135e-01, 4.5407e-01,\n",
            "          6.1878e-01, 3.7510e-01, 4.6952e-01, 9.0628e-01, 1.2137e-01,\n",
            "          4.6831e-01, 8.6466e-01, 5.0445e-01, 4.7490e-01, 3.6332e-01,\n",
            "          5.1044e-01, 1.6055e-01, 5.6264e-01]]], device='cuda:0')\n",
            "logits tensor([[-0.0770, -0.0072,  0.0498, -0.0488,  0.0785, -0.0285,  0.1266, -0.0438,\n",
            "         -0.0235,  0.0299]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "pred_probab tensor([[0.0919, 0.0985, 0.1043, 0.0945, 0.1074, 0.0965, 0.1127, 0.0950, 0.0970,\n",
            "         0.1023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "y_pred tensor([6], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(1, 28, 28, device=device)\n",
        "print(\"x\",x)\n",
        "logits = model(x)\n",
        "print(\"logits\",logits)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "print(\"pred_probab\",pred_probab)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(\"y_pred\",y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7r_1RJQDjEc"
      },
      "source": [
        "由上面可以看出来预测的变化，经过分类器之后，可以得到概率最大的那个类别，也就是随即的类别。**但是这里其实并没有真正的训练，只是将随即的变量代入了模型，模型内部的所有的参数都是初始化来的，并不是学习来的。**，此处只是简单的建立了模型，模型的训练在后面。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oVJ2xNTEPLp"
      },
      "source": [
        "# 模型层次详解\n",
        "\n",
        "## nn.Flatten层\n",
        "顾名思义，这是一个扁平化层，是将输入的图像，本来是三个通道每个通道的有`28*28`个像素的图片，现在将这个28*28个像素展开，即每个通道都展开，这样做的好处就是一次性可以将所有的像素输入进去。\n",
        "\n",
        "## nn.Linear 层\n",
        "这是一个将线性转换器应用到模型的一层，利用其自身存储好的权重(weight)和偏移值(bias),来\n",
        "进行线性计算。\n",
        "\n",
        "## nn.ReLU层\n",
        "这是非线性的激活函数，用于创建复杂的输入与输出之间的映射。放在了线性模型的输入之后，目的是引入非线性，帮助神经网络模型来学习更加丰富多彩的现象。\n",
        "\n",
        "## nn.Sequential 层\n",
        "这是一个有序的模型容器。输入数据被放入这个序列中，依次按照序列的定义来对数据进行操作。可以使用Sequential容器来快捷的创立一个模型。\n",
        "\n",
        "## nn.Softmax 层\n",
        "模型的上一层输出的是一个logits tensor，是一个介于无穷小到无穷大之间的数字。这个数字我们没有办法拿来预测，因此需要将这些值按比例缩小到[0,1]之间，并且每一个类别的概率之和最终等于一，便可以利用概率的知识，来判断预测的类别。其中的`dim=1`表示的就是最终的值的和必须符合的维度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0etEa0kDSKB",
        "outputId": "f8e10190-3d4d-46c6-8407-ff9589d41ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n",
            "torch.Size([3, 784])\n",
            "torch.Size([3, 20])\n",
            "torch.Size([3, 20])\n",
            "tensor([[-0.0143, -0.3681, -0.3948, -0.1156,  0.1930, -0.0073, -0.0252, -0.0910,\n",
            "          0.2857,  0.1458],\n",
            "        [-0.1536, -0.2145, -0.3593, -0.2224, -0.1062,  0.1601,  0.0647, -0.1182,\n",
            "          0.2194,  0.1233],\n",
            "        [-0.0449, -0.2724, -0.2270, -0.3281,  0.0094,  0.0651, -0.0648,  0.0307,\n",
            "          0.2427,  0.1028]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[0.1003, 0.0704, 0.0686, 0.0907, 0.1235, 0.1011, 0.0993, 0.0929, 0.1355,\n",
            "         0.1178],\n",
            "        [0.0896, 0.0843, 0.0730, 0.0837, 0.0940, 0.1227, 0.1115, 0.0929, 0.1302,\n",
            "         0.1182],\n",
            "        [0.0989, 0.0788, 0.0825, 0.0745, 0.1045, 0.1104, 0.0970, 0.1067, 0.1319,\n",
            "         0.1147]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 模型层次详解测试\n",
        "input_data = torch.rand(3,28,28)\n",
        "print(input_data.size())\n",
        "\n",
        "# 展开层\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_data)\n",
        "print(flat_image.size())\n",
        "\n",
        "# 线性层\n",
        "linear = nn.Linear(28*28,20)\n",
        "hidden = linear(flat_image)\n",
        "print(hidden.size())\n",
        "\n",
        "# ReLU层\n",
        "hidden = nn.ReLU()(hidden)\n",
        "print(hidden.size())\n",
        "\n",
        "# Sequential 容器\n",
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    linear,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)\n",
        "print(logits)\n",
        "\n",
        "## softmax 层\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "print(pred_probab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6DzcmgOMAKO"
      },
      "source": [
        "# 模型参数\n",
        "在模型的显式训练的过程中，是有很多的参数的。我们可以将这些参数打印出来，nn.Module会自动跟踪定义的所有的模型，并保存其参数值。可以使用模型的`parameters()`或者`named_parameters()`方法来获取模型的参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dYQVPn7L5M3",
        "outputId": "66622a1e-7d61-461a-ded8-5b464d0a9275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0067, -0.0107, -0.0082,  ...,  0.0005,  0.0229,  0.0243],\n",
            "        [-0.0280, -0.0118,  0.0119,  ...,  0.0350,  0.0310,  0.0104]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0112, 0.0276], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 2.1209e-02, -3.7238e-02,  1.8224e-02,  ...,  2.9277e-02,\n",
            "         -2.1018e-02,  4.3991e-05],\n",
            "        [-2.8605e-02,  5.1087e-03,  2.6520e-03,  ..., -2.8026e-02,\n",
            "          5.0956e-03,  1.2085e-02]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0240, -0.0273], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0028, -0.0247, -0.0213,  ...,  0.0420, -0.0060,  0.0198],\n",
            "        [-0.0403, -0.0106, -0.0080,  ..., -0.0238, -0.0380,  0.0212]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0440,  0.0260], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk4ZA1FJMnd6"
      },
      "source": [
        "over"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BuildModule.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5f2dd485db4a25afbf62c4235ab875a3fea3f8e9f4ee38ce521033e7c3b9a87d"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('.env': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
