{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BuildModule.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5+BiNvy/TyU1Kia8KzHfL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HvF0RHQ7_V5Q"},"source":["# 神经网络\n","\n","将对于数据的操作的各个模块或层次组织在一起，就是神经网络模型。`torch.nn`模块提供了所有的用于建立神经网络模型的模块的命名空间(隐藏层、输出层等)。所有的模型都是`nn.Moudle`的子类。一个神经网络模型可以包括许多的其他的模型，这样就可以建立很复杂的结构。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jOKaRrX_SDd","executionInfo":{"status":"ok","timestamp":1635077001743,"user_tz":-480,"elapsed":25268,"user":{"displayName":"D Jeffrey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13048221212790957836"}},"outputId":"590cdb03-4fff-40e9-f61d-63f57bd7a388"},"source":["# 导入相应的包\n","import torch\n","import os\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# 获取可以执行的设备\n","print(\"计算运行设备:\", device)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["计算运行设备: cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"mLj1AMvF_b_e"},"source":["# 建立模型\n","可以通过继承`nn.Module`类来实现模型的建立，然后在`__init__`函数里初始化建立模型。所有的Module子类都需要实现前向传播函数`forward`,来实现对输入数据的处理。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvlM7wqj_aAq","executionInfo":{"status":"ok","timestamp":1635077016878,"user_tz":-480,"elapsed":15143,"user":{"displayName":"D Jeffrey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13048221212790957836"}},"outputId":"347f5c86-ab7c-4130-b77e-4d515d558ef1"},"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork,self).__init__()\n","        self.flatten = nn.Flatten()   # 将图像扁平化\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28,512),\n","            nn.ReLU(),\n","            nn.Linear(512,512),\n","            nn.ReLU(),\n","            nn.Linear(512,10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)  # 获取每一batch的输入，并利用第一个模块扁平化\n","        logits = self.linear_relu_stack(x) # 放入第二个复杂的模型进行训练\n","        return logits\n","    \n","# 将模型放到对应的结构上\n","model = NeuralNetwork().to(device)\n","print(model)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"nDsjjmv6B0dQ"},"source":["# 使用模型\n","为了使用模型，可以直接向模型传递输入数据，上面的模型最后将会输出一个10维的`tensor`结构，这些操作都是在后台进行的，无需显式的调用`forward`函数进行计算。因为这是一个多分类问题，将上述的10维的结构放入一个`softmax`分类器里面，就可以得到概率最大的那个类别，也就是最终的分类。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9K6kp62kBavp","executionInfo":{"status":"ok","timestamp":1635077016879,"user_tz":-480,"elapsed":15,"user":{"displayName":"D Jeffrey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13048221212790957836"}},"outputId":"da8e920d-1bf8-43d1-9d2d-1b962a26b64c"},"source":["x = torch.rand(1, 28, 28, device=device)\n","print(\"x\",x)\n","logits = model(x)\n","print(\"logits\",logits)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","print(\"pred_probab\",pred_probab)\n","y_pred = pred_probab.argmax(1)\n","print(\"y_pred\",y_pred)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["x tensor([[[7.8331e-01, 4.0430e-01, 1.5309e-01, 9.0927e-01, 6.9913e-01,\n","          4.6765e-01, 9.0149e-01, 7.4656e-01, 4.8227e-01, 2.0388e-01,\n","          8.5696e-01, 1.5308e-01, 2.7565e-01, 2.7932e-01, 7.2258e-01,\n","          1.5606e-01, 4.0048e-01, 6.8498e-01, 3.0512e-01, 5.2559e-01,\n","          6.0010e-01, 5.2044e-01, 8.6385e-01, 6.7460e-01, 1.9618e-01,\n","          7.0253e-01, 3.0140e-01, 3.8909e-01],\n","         [6.2238e-01, 2.8826e-01, 7.0873e-01, 4.3049e-01, 6.6013e-01,\n","          6.5122e-02, 8.0897e-01, 5.9807e-01, 9.5499e-01, 3.7352e-02,\n","          3.6375e-01, 9.9677e-01, 4.2074e-01, 7.6707e-01, 6.4430e-02,\n","          6.0024e-01, 6.3686e-01, 2.7674e-01, 4.5303e-01, 7.5484e-01,\n","          7.7232e-01, 9.8329e-01, 1.0953e-01, 3.7313e-01, 3.8552e-01,\n","          4.4290e-01, 8.3544e-01, 5.6694e-01],\n","         [3.3093e-01, 9.1029e-01, 9.3298e-01, 3.1736e-01, 3.8311e-01,\n","          5.5716e-01, 9.6307e-01, 3.4901e-01, 1.6077e-01, 5.8234e-01,\n","          4.6331e-01, 9.1416e-01, 8.7926e-01, 9.0364e-01, 6.5101e-01,\n","          9.2866e-01, 9.3932e-01, 8.7083e-01, 4.8276e-02, 1.9060e-01,\n","          9.8655e-01, 3.9384e-01, 9.3177e-01, 9.3746e-02, 1.3065e-01,\n","          6.3191e-01, 9.5743e-01, 4.9818e-01],\n","         [6.0494e-01, 6.6903e-01, 5.7503e-01, 7.4828e-02, 7.6472e-01,\n","          8.7435e-01, 4.9178e-01, 9.5893e-01, 5.4059e-02, 4.6193e-01,\n","          8.3418e-01, 1.7988e-02, 6.1948e-03, 9.6230e-01, 4.6327e-01,\n","          5.2204e-04, 5.3271e-01, 2.6979e-02, 3.2758e-01, 7.1449e-01,\n","          3.9244e-01, 9.9397e-01, 4.6017e-01, 6.4614e-01, 6.2103e-01,\n","          1.1280e-01, 3.8313e-01, 8.4737e-01],\n","         [5.5770e-01, 3.3337e-01, 1.4669e-01, 1.5942e-02, 3.5292e-01,\n","          2.6910e-01, 7.9622e-01, 9.5460e-01, 5.9473e-01, 2.0965e-01,\n","          5.7315e-01, 6.9385e-01, 3.5293e-01, 9.3269e-01, 9.7491e-01,\n","          5.0345e-01, 5.6683e-01, 2.6887e-01, 9.9562e-01, 3.6836e-01,\n","          1.3865e-01, 5.8357e-01, 5.1660e-01, 3.3083e-01, 4.1840e-02,\n","          2.3799e-01, 7.4782e-01, 1.9130e-01],\n","         [3.3479e-01, 9.0778e-02, 9.6732e-01, 7.2948e-01, 2.8140e-01,\n","          8.2660e-01, 1.4283e-01, 8.6646e-01, 6.3279e-01, 9.9253e-02,\n","          5.6929e-01, 5.7453e-01, 8.3376e-01, 7.7835e-01, 1.6222e-01,\n","          5.1143e-03, 1.1568e-01, 6.4990e-01, 1.4331e-01, 4.7997e-01,\n","          6.7586e-02, 3.5382e-01, 3.9648e-01, 4.4973e-01, 2.7160e-02,\n","          8.6274e-02, 2.8535e-01, 6.1905e-01],\n","         [5.0610e-01, 1.4009e-01, 3.3014e-01, 2.1632e-01, 6.5503e-01,\n","          2.7451e-01, 1.5892e-01, 1.6207e-02, 9.5473e-01, 4.7030e-01,\n","          7.3165e-01, 6.0980e-01, 5.2530e-02, 7.6023e-01, 7.8077e-01,\n","          8.1127e-01, 2.1505e-01, 6.3631e-01, 1.6774e-01, 9.5899e-01,\n","          7.8192e-01, 9.6919e-01, 2.8839e-01, 7.6939e-01, 8.8231e-01,\n","          8.4974e-01, 2.6448e-01, 6.1247e-01],\n","         [9.1528e-01, 6.8530e-01, 6.3656e-01, 5.4156e-01, 4.9074e-01,\n","          1.1722e-01, 1.8043e-01, 4.2330e-01, 2.8769e-01, 6.6460e-01,\n","          5.2834e-01, 9.7458e-01, 7.9186e-01, 9.8185e-01, 3.8491e-01,\n","          2.9810e-01, 5.7160e-01, 8.0352e-01, 3.4369e-01, 6.6882e-01,\n","          3.9355e-01, 3.1264e-01, 7.9901e-01, 5.7810e-01, 7.2690e-01,\n","          4.7538e-01, 5.2735e-01, 7.4822e-01],\n","         [8.8323e-01, 7.5690e-02, 1.3363e-01, 7.9377e-01, 3.4476e-01,\n","          8.8291e-01, 8.3747e-01, 4.0401e-01, 9.1821e-01, 4.1168e-01,\n","          6.0876e-01, 7.7049e-01, 8.3978e-01, 4.1919e-01, 2.3670e-01,\n","          9.6797e-01, 3.5363e-02, 4.5823e-01, 3.8554e-01, 8.8103e-01,\n","          5.2304e-01, 5.4030e-01, 4.3754e-01, 1.1976e-01, 3.4617e-01,\n","          1.1496e-01, 6.3787e-01, 7.2519e-01],\n","         [8.6816e-01, 8.0506e-01, 1.5593e-02, 5.2897e-01, 8.6897e-01,\n","          7.5937e-01, 2.3352e-01, 2.8021e-01, 1.6925e-01, 6.8982e-01,\n","          9.0259e-01, 7.6705e-01, 3.5176e-01, 9.0516e-01, 5.4145e-01,\n","          9.4770e-01, 9.8400e-01, 3.4201e-02, 3.6982e-02, 9.0609e-01,\n","          6.6218e-01, 9.9529e-01, 1.6357e-01, 2.9497e-01, 4.4778e-01,\n","          6.2542e-01, 5.9665e-01, 6.9166e-01],\n","         [1.7972e-01, 3.8323e-01, 5.1417e-01, 9.4765e-01, 2.5926e-01,\n","          5.1102e-01, 9.6450e-01, 4.2125e-01, 4.0655e-01, 3.0247e-01,\n","          9.4177e-01, 6.4241e-01, 8.8249e-01, 4.6680e-01, 2.5739e-01,\n","          9.8894e-01, 8.6754e-01, 7.6380e-01, 6.0648e-01, 5.9430e-01,\n","          1.5826e-03, 1.5324e-01, 8.2569e-01, 7.1273e-03, 5.8924e-01,\n","          8.0066e-01, 6.3761e-01, 8.6818e-01],\n","         [7.7860e-01, 3.7233e-01, 1.9287e-01, 1.1356e-02, 6.3772e-01,\n","          6.0351e-01, 8.7604e-01, 8.6702e-02, 2.0932e-01, 5.9155e-01,\n","          7.8277e-01, 4.2971e-01, 7.2529e-01, 6.3725e-01, 8.8379e-02,\n","          5.0947e-01, 6.4918e-01, 1.3047e-01, 6.7972e-01, 3.2177e-01,\n","          3.8704e-01, 5.0146e-01, 3.8930e-01, 4.8090e-01, 4.9471e-01,\n","          8.8386e-02, 1.4797e-02, 3.7768e-01],\n","         [3.4405e-01, 5.3068e-01, 5.8166e-01, 3.1016e-01, 4.0233e-01,\n","          7.7093e-01, 9.4654e-01, 6.7709e-01, 7.4618e-01, 7.4458e-01,\n","          4.4915e-01, 3.3460e-02, 4.4262e-01, 2.0446e-01, 8.4367e-02,\n","          5.5501e-01, 8.7678e-01, 2.3170e-01, 1.1022e-01, 5.5853e-01,\n","          8.5599e-01, 2.1887e-01, 5.3536e-01, 3.6218e-01, 1.2474e-01,\n","          9.5126e-01, 8.4112e-01, 3.6632e-01],\n","         [3.3613e-01, 2.2817e-01, 3.4356e-01, 9.5361e-01, 3.2493e-01,\n","          4.9278e-01, 8.3147e-01, 5.6196e-01, 5.8964e-01, 7.4568e-01,\n","          6.8422e-01, 9.3192e-01, 1.2015e-01, 2.9474e-01, 3.2660e-01,\n","          9.8115e-01, 5.9317e-01, 3.8270e-01, 7.2884e-01, 9.5055e-01,\n","          2.9559e-01, 3.7886e-01, 7.2380e-01, 8.7157e-01, 7.1805e-02,\n","          9.7133e-01, 4.0876e-01, 5.2748e-01],\n","         [9.8583e-01, 9.8874e-01, 4.4562e-01, 3.0601e-01, 8.6918e-01,\n","          1.1034e-02, 1.3361e-01, 4.8335e-01, 6.5053e-02, 5.2518e-01,\n","          6.9781e-01, 7.4009e-01, 6.6953e-01, 7.6036e-01, 5.7052e-01,\n","          9.4542e-01, 5.8428e-01, 8.8944e-01, 2.2781e-01, 9.3416e-01,\n","          9.0232e-01, 5.3371e-01, 6.3360e-01, 5.5534e-01, 1.1587e-01,\n","          3.1482e-01, 9.1888e-01, 6.1016e-01],\n","         [8.1759e-01, 9.1529e-01, 3.9381e-01, 9.6895e-01, 1.2442e-01,\n","          9.4904e-01, 5.4896e-01, 6.5680e-01, 9.1665e-02, 5.9786e-01,\n","          5.3433e-01, 7.5605e-01, 4.6578e-01, 3.8427e-01, 6.1184e-02,\n","          3.0380e-01, 3.1675e-01, 5.9639e-01, 2.1781e-01, 2.7625e-01,\n","          7.2003e-01, 3.6892e-01, 1.0970e-01, 7.6653e-01, 3.2763e-01,\n","          4.1210e-01, 1.6179e-01, 1.5287e-01],\n","         [7.5896e-01, 7.8303e-01, 4.6500e-01, 8.1631e-01, 4.4217e-01,\n","          3.2823e-01, 3.1312e-01, 2.8142e-01, 8.5555e-01, 4.9904e-01,\n","          2.6501e-01, 7.4552e-01, 9.1530e-01, 1.4164e-01, 9.9284e-03,\n","          3.9422e-01, 6.6370e-02, 9.7897e-01, 7.2140e-01, 1.3189e-01,\n","          1.5338e-01, 8.3662e-01, 2.8906e-01, 6.2822e-01, 1.2160e-01,\n","          7.2286e-01, 7.2480e-01, 9.3649e-01],\n","         [3.5627e-01, 8.7722e-01, 1.0689e-01, 3.3710e-01, 9.0261e-01,\n","          5.0845e-02, 5.1829e-01, 5.5519e-01, 6.3689e-01, 3.2615e-01,\n","          7.2717e-01, 2.2186e-01, 7.5092e-01, 2.2920e-01, 4.4582e-01,\n","          6.5807e-01, 8.0695e-01, 8.3029e-01, 7.0412e-01, 5.8055e-01,\n","          9.6309e-01, 7.5005e-01, 6.8126e-02, 8.6905e-01, 5.3638e-01,\n","          7.9243e-01, 6.7191e-01, 5.0175e-01],\n","         [9.7813e-01, 2.5354e-01, 2.2086e-01, 5.0868e-02, 2.8126e-01,\n","          2.5370e-01, 1.8703e-01, 9.4079e-01, 8.9873e-01, 4.0519e-01,\n","          1.2708e-01, 1.8391e-01, 9.2725e-01, 3.8548e-01, 8.1259e-02,\n","          9.5974e-01, 3.6649e-01, 2.6942e-02, 7.2434e-03, 3.8287e-01,\n","          4.0259e-01, 3.6524e-01, 5.9424e-01, 9.9688e-01, 2.2014e-01,\n","          4.0431e-01, 7.9652e-01, 9.2505e-01],\n","         [2.1191e-01, 5.6729e-01, 6.2460e-01, 4.4450e-01, 4.4449e-01,\n","          6.4131e-01, 3.4174e-01, 7.5322e-01, 8.1754e-01, 4.7387e-01,\n","          4.8189e-01, 7.0884e-01, 1.8604e-01, 9.8938e-01, 2.5590e-01,\n","          1.2466e-01, 9.7874e-01, 8.7722e-01, 8.0824e-01, 9.3443e-01,\n","          9.0036e-02, 7.7660e-01, 7.7385e-01, 1.9578e-01, 4.0143e-01,\n","          1.8378e-01, 7.2033e-02, 2.7317e-01],\n","         [7.0510e-01, 4.4784e-01, 2.6477e-01, 9.6966e-01, 7.6152e-01,\n","          7.2594e-01, 6.6652e-01, 6.8766e-01, 4.4289e-01, 6.9376e-01,\n","          5.7792e-01, 9.5183e-01, 2.4878e-01, 7.4029e-01, 3.9566e-01,\n","          7.1785e-01, 6.4254e-01, 8.5272e-01, 8.3670e-01, 5.5841e-01,\n","          5.1655e-02, 2.7042e-01, 8.2775e-01, 5.9367e-01, 5.1359e-01,\n","          9.6319e-01, 4.1375e-01, 7.8815e-02],\n","         [2.3108e-01, 7.1861e-01, 4.9375e-02, 1.7554e-01, 8.5529e-01,\n","          8.7950e-01, 4.0835e-01, 8.4120e-02, 2.5906e-01, 3.9038e-01,\n","          2.5857e-01, 2.8682e-02, 7.1209e-01, 1.7853e-01, 3.7015e-01,\n","          3.0722e-01, 6.7260e-01, 1.1376e-01, 8.6074e-01, 5.3319e-01,\n","          3.6036e-01, 7.4852e-01, 3.5344e-01, 5.7184e-01, 4.8177e-01,\n","          4.7862e-01, 3.1183e-01, 3.0188e-01],\n","         [6.1757e-01, 5.7917e-01, 3.4102e-01, 6.4109e-01, 1.4326e-01,\n","          2.4274e-01, 8.0220e-01, 9.7037e-01, 9.2754e-01, 6.9276e-01,\n","          4.2542e-01, 4.6938e-01, 4.7296e-02, 4.6946e-01, 6.5306e-02,\n","          2.3010e-01, 3.3633e-01, 3.4502e-01, 6.7184e-01, 6.8946e-01,\n","          2.2464e-01, 8.7891e-01, 8.3917e-01, 8.3900e-01, 6.3061e-01,\n","          6.6032e-01, 8.6023e-01, 9.4457e-01],\n","         [5.5250e-01, 6.6782e-01, 1.3100e-02, 1.6308e-01, 2.4081e-01,\n","          7.2240e-01, 5.4244e-01, 2.8325e-01, 2.3234e-01, 8.0956e-01,\n","          2.3758e-01, 8.6163e-01, 4.5883e-01, 5.6452e-01, 2.7418e-01,\n","          5.4847e-01, 7.0987e-01, 3.3339e-01, 5.4054e-01, 7.3799e-01,\n","          6.6109e-01, 5.2794e-01, 1.4413e-01, 6.7755e-01, 4.0210e-01,\n","          8.6549e-01, 8.2505e-01, 6.5040e-01],\n","         [7.1982e-01, 1.4052e-01, 6.9366e-01, 8.2975e-01, 7.2008e-01,\n","          9.8929e-01, 8.3850e-01, 6.6019e-01, 1.7030e-01, 3.0162e-01,\n","          2.9766e-01, 7.3498e-01, 5.5581e-01, 6.0877e-01, 1.3277e-01,\n","          9.3030e-01, 9.3828e-01, 9.0424e-01, 2.9555e-01, 1.0978e-01,\n","          9.8765e-01, 7.9875e-01, 4.4015e-02, 5.7048e-01, 1.9261e-01,\n","          6.7197e-01, 9.2899e-01, 9.9246e-01],\n","         [2.7021e-01, 4.8455e-01, 5.1558e-01, 6.6670e-01, 8.5387e-01,\n","          5.1305e-01, 1.6812e-01, 8.0839e-01, 5.7086e-01, 8.8062e-02,\n","          1.8466e-01, 5.2827e-01, 4.3122e-01, 1.4125e-01, 6.9700e-01,\n","          3.5258e-01, 6.0605e-01, 4.8733e-01, 3.3913e-01, 7.3326e-02,\n","          7.3594e-01, 1.2487e-01, 8.5565e-01, 3.1912e-01, 9.6582e-01,\n","          7.2788e-01, 6.2763e-01, 8.3745e-02],\n","         [4.9478e-02, 1.2444e-01, 5.4959e-01, 9.2737e-01, 8.2838e-01,\n","          7.0074e-01, 4.1085e-01, 2.4572e-01, 8.6305e-01, 9.9929e-01,\n","          9.1926e-01, 4.0050e-01, 4.6431e-01, 8.5760e-01, 3.4151e-01,\n","          3.7543e-01, 2.8091e-01, 5.7121e-01, 1.6911e-01, 4.1623e-01,\n","          5.3579e-01, 1.4161e-01, 8.3759e-01, 7.9707e-01, 1.6046e-01,\n","          5.0280e-01, 1.3578e-01, 1.0507e-02],\n","         [7.1776e-01, 7.7072e-01, 1.5965e-01, 1.6808e-01, 4.8966e-01,\n","          8.0254e-01, 5.8705e-01, 1.8389e-01, 2.0841e-01, 8.2239e-01,\n","          4.0307e-01, 1.2945e-01, 4.0532e-01, 2.0076e-01, 4.8578e-01,\n","          9.6337e-02, 2.3604e-01, 2.2680e-01, 9.3873e-01, 1.2206e-01,\n","          2.3932e-02, 1.9182e-01, 7.5459e-01, 2.8331e-02, 3.7102e-01,\n","          1.6206e-01, 7.2794e-01, 1.7371e-01]]], device='cuda:0')\n","logits tensor([[ 0.0428, -0.0080,  0.0265, -0.1019, -0.1492, -0.0090, -0.0108,  0.0968,\n","         -0.1275, -0.0231]], device='cuda:0', grad_fn=<AddmmBackward>)\n","pred_probab tensor([[0.1069, 0.1016, 0.1051, 0.0925, 0.0882, 0.1015, 0.1013, 0.1128, 0.0901,\n","         0.1000]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n","y_pred tensor([7], device='cuda:0')\n"]}]},{"cell_type":"markdown","metadata":{"id":"k7r_1RJQDjEc"},"source":["由上面可以看出来预测的变化，经过分类器之后，可以得到概率最大的那个类别，也就是随即的类别。**但是这里其实并没有真正的训练，只是将随即的变量代入了模型，模型内部的所有的参数都是初始化来的，并不是学习来的。**，此处只是简单的建立了模型，模型的训练在后面。"]},{"cell_type":"markdown","metadata":{"id":"0oVJ2xNTEPLp"},"source":["# 模型层次详解\n","\n","## nn.Flatten层\n","顾名思义，这是一个扁平化层，是将输入的图像，本来是三个通道每个通道的有`28*28`个像素的图片，现在将这个28*28个像素展开，即每个通道都展开，这样做的好处就是一次性可以将所有的像素输入进去。\n","\n","## nn.Linear 层\n","这是一个将线性转换器应用到模型的一层，利用其自身存储好的权重(weight)和偏移值(bias),来\n","进行线性计算。\n","\n","## nn.ReLU层\n","这是非线性的激活函数，用于创建复杂的输入与输出之间的映射。放在了线性模型的输入之后，目的是引入非线性，帮助神经网络模型来学习更加丰富多彩的现象。\n","\n","## nn.Sequential 层\n","这是一个有序的模型容器。输入数据被放入这个序列中，依次按照序列的定义来对数据进行操作。可以使用Sequential容器来快捷的创立一个模型。\n","\n","## nn.Softmax 层\n","模型的上一层输出的是一个logits tensor，是一个介于无穷小到无穷大之间的数字。这个数字我们没有办法拿来预测，因此需要将这些值按比例缩小到[0,1]之间，并且每一个类别的概率之和最终等于一，便可以利用概率的知识，来判断预测的类别。其中的`dim=1`表示的就是最终的值的和必须符合的维度。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0etEa0kDSKB","executionInfo":{"status":"ok","timestamp":1635077056215,"user_tz":-480,"elapsed":16,"user":{"displayName":"D Jeffrey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13048221212790957836"}},"outputId":"f8e10190-3d4d-46c6-8407-ff9589d41ab8"},"source":["# 模型层次详解测试\n","input_data = torch.rand(3,28,28)\n","print(input_data.size())\n","\n","# 展开层\n","flatten = nn.Flatten()\n","flat_image = flatten(input_data)\n","print(flat_image.size())\n","\n","# 线性层\n","linear = nn.Linear(28*28,20)\n","hidden = linear(flat_image)\n","print(hidden.size())\n","\n","# ReLU层\n","hidden = nn.ReLU()(hidden)\n","print(hidden.size())\n","\n","# Sequential 容器\n","seq_modules = nn.Sequential(\n","    flatten,\n","    linear,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)\n","print(logits)\n","\n","## softmax 层\n","softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)\n","print(pred_probab)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n","torch.Size([3, 784])\n","torch.Size([3, 20])\n","torch.Size([3, 20])\n","tensor([[ 0.1568, -0.0936, -0.0447,  0.3378,  0.0892, -0.0914,  0.3255, -0.1408,\n","          0.3747,  0.1576],\n","        [ 0.1487,  0.0172, -0.0119,  0.2713,  0.0475, -0.0244,  0.3693, -0.1314,\n","          0.3427,  0.1524],\n","        [ 0.1345, -0.0866,  0.0135,  0.3104,  0.0996, -0.1466,  0.2608, -0.1119,\n","          0.4364,  0.0819]], grad_fn=<AddmmBackward>)\n","tensor([[0.1033, 0.0804, 0.0845, 0.1238, 0.0966, 0.0806, 0.1223, 0.0767, 0.1285,\n","         0.1034],\n","        [0.1018, 0.0892, 0.0867, 0.1151, 0.0920, 0.0856, 0.1269, 0.0769, 0.1236,\n","         0.1022],\n","        [0.1019, 0.0816, 0.0902, 0.1214, 0.0984, 0.0769, 0.1156, 0.0796, 0.1378,\n","         0.0966]], grad_fn=<SoftmaxBackward>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"z6DzcmgOMAKO"},"source":["# 模型参数\n","在模型的显式训练的过程中，是有很多的参数的。我们可以将这些参数打印出来，nn.Module会自动跟踪定义的所有的模型，并保存其参数值。可以使用模型的`parameters()`或者`named_parameters()`方法来获取模型的参数。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dYQVPn7L5M3","executionInfo":{"status":"ok","timestamp":1635077212849,"user_tz":-480,"elapsed":787,"user":{"displayName":"D Jeffrey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13048221212790957836"}},"outputId":"66622a1e-7d61-461a-ded8-5b464d0a9275"},"source":["print(\"Model structure: \", model, \"\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure:  NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",") \n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0292,  0.0343,  0.0295,  ..., -0.0068, -0.0105,  0.0169],\n","        [-0.0252,  0.0179,  0.0225,  ...,  0.0110, -0.0297,  0.0067]],\n","       device='cuda:0', grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0129, 0.0053], device='cuda:0', grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0431, -0.0155,  0.0387,  ..., -0.0185, -0.0389, -0.0313],\n","        [ 0.0140, -0.0137,  0.0157,  ...,  0.0333, -0.0100,  0.0283]],\n","       device='cuda:0', grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0078, -0.0106], device='cuda:0', grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0227,  0.0391,  0.0149,  ..., -0.0194,  0.0213, -0.0165],\n","        [ 0.0335, -0.0274, -0.0276,  ...,  0.0017,  0.0285,  0.0373]],\n","       device='cuda:0', grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0126,  0.0044], device='cuda:0', grad_fn=<SliceBackward>) \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zk4ZA1FJMnd6"},"source":["over"]}]}